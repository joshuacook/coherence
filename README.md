# Mutual Information Assessment of Low Rank Representations

This work is based upon topic model work in the domain of natural language processing. A Topic Model (TM) is a **low-rank representation** of a word co-occurrence matrix often generated by a matrix factorization such as a Singular Value Decomposition (SVD) or a Non-Negative Matrix Factorization (NMF).

## Topic Coherence
Topic Coherence is a metric for automatically assessing the quality of such a topic model. Intrinsic [[2011mimno]] and extrinsic [[2010newman]] methods exist for programmatically computing Topic Coherence as semantically coherent concepts.

## Principal Component Analysis
A TM is prepared so that if _X_ is the word co-occurrence matrix, then via SVD

<img width=150px src="https://render.githubusercontent.com/render/math?math=X = U\Sigma V^T">

where _U_ is an embedding of the original documents in a low-rank space, _Σ_ is the singular values (similar to eigenvalues and the explained variance of each topic), and _V_ is the eigenvectors of the co-occurrence matrix.

A  Principal Component Analysis (PCA) is a low-rank representation of a data set of continuous features. It is often generated by the same matrix factorization techniques used to generate a TM, where _U_ is an embedding of the original observations in a low-rank space, _Σ_ is the singular values (similar to eigenvalues and the explained variance of each topic), and _V_ is the eigenvectors of the covariance (or correlation) matrix, the **Loadings Matrix**.

A single topic of a TM is an eigenvector of the word co-occurrence matrix. A single vector of the Loadings Matrix describes a dominant axis in the data according to the covariance. The plot below shows a plot of the loadings of the first two principal components for a PCA generated on the Iris Dataset [[irisOnUCIMLrepo]]. The PCA on the left was generated using the covariance matrix. The PCA on the right was generated using the correlation matrix, that is the data was scaled by subtracting the mean and dividing by the standard deviation.

Note that this plot demonstrates a common issue in the preparation of a PCA model. Without scaling the data, a single feature has come to dominate the first principal component. This observation is made in an adhoc manner.

![](https://www.evernote.com/l/AAGTlcEFwFNDx4caIES-mFvClrd2-Ta1EOEB/image.png)

### Coherence and PCA

For a given topic from a TM, Coherence is used to assess the quality of that topic. In other words, each vector of <em>V<sup>T</sup></em> is given a Coherence. This work seeks to establish a method for providing a Coherence for each vector of <em>V<sup>T</sup></em> generated during the preparation of a PCA model.

It is of note that up to some small variation caused by the possible use of randomized processes in the preparation of an SVD, the generation of a PCA Model and its Loading Matrix is deterministic. Because of this, it is not possible to tune a PCA model in the conventional sense. Rather different PCA Models can be generated by preparing the data passed to the SVD in different ways, as demonstrated above when the PCA Model is prepared both with and without scaling the input data.

In this paper, we hope to establish an automated method for assessing the quality of the principal component vectors generated for a given model. Trivially, we would hope PC 1 for the scaled Iris data would be more coherent than PC 1 for the non-scaled data, but we also hope to establish a general method than can be used to assess principal components generated by other data preparation methods.

## Computing Coherence

Intrinsic coherence [[2011mimno]] is generated using the following computation

<img width=400px src="https://render.githubusercontent.com/render/math?math=C\left(t,V^{(t)}\right) = \sum_{m=2}^M\sum_{l=1}^{m-1}\log \frac{D\left(v_m^{(t)}, v_l^{(t)}\right) + \gamma}{D\left(v_l^{(t)}\right)}">

Here _D(v)_ is the *document frequency* of a word, _D(v,v')_ is the co-document frequency of two words, <em>V<sup>(t)</sup></em> is the set of the _M_ most probable words in topic _t_, and γ is a smoothing constant, often 1, used to prevent taking the log of 0. 

[2011mimno]: https://dl.acm.org/citation.cfm?id=2145462
[2010newman]: https://dl.acm.org/citation.cfm?id=1858011
[irisOnUCIMLrepo]: https://archive.ics.uci.edu/ml/datasets/iris
